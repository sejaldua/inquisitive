{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.4.0-cp38-cp38-macosx_11_0_arm64.whl (761 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.6/761.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /Users/sejaldua/Library/Python/3.8/lib/python/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/sejaldua/Library/Python/3.8/lib/python/site-packages (from tiktoken) (2023.8.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sejaldua/Library/Python/3.8/lib/python/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sejaldua/Library/Python/3.8/lib/python/site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sejaldua/Library/Python/3.8/lib/python/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sejaldua/Library/Python/3.8/lib/python/site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/sejaldua/Desktop/misc/llm-chatbot', '/Users/sejaldua/Desktop/misc/sport-reference-scraping', '/Users/sejaldua/.vscode/extensions/ms-toolsai.jupyter-2022.5.1001601848/pythonFiles', '/Users/sejaldua/.vscode/extensions/ms-toolsai.jupyter-2022.5.1001601848/pythonFiles/lib/python', '/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python38.zip', '/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8', '/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/lib-dynload', '', '/Users/sejaldua/Library/Python/3.8/lib/python/site-packages', '/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sejaldua/Library/Python/3.8/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sejaldua/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "#import magic\n",
    "import os\n",
    "import nltk\n",
    "import config\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# pip install unstructured\n",
    "# Other dependencies to install https://langchain.readthedocs.io/en/latest/modules/document_loaders/examples/unstructured_file.html\n",
    "# pip install python-magic-bin\n",
    "# pip install chromadb\n",
    "# pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('doc/')\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"About the job\\n\\nAbout The Team\\n\\nZillow Group’s mission is to give people the power to unlock life's next chapter. The Customer Conversion Analytics team builds models that enable smarter products and deliver strategic insights that help customers transition from browsing to successfully buying or selling a home.\\n\\nZillow, the top real estate website in the U.S., is building an on-demand real estate experience. Whether selling, buying, renting or financing, customers can turn to Zillow to find and get into their next home with speed, certainty and ease.\\n\\nAbout The Role\\n\\nThis role will employ a broad variety of data science skills, all with the aim of creating a fantastic product that supports Flex agents who partner with Zillow via revenue sharing to help buyers navigate their home-buying journey. This role helps understand questions such as:\\n\\nReal estate transactions are long processes with a large range of possible duration to transaction. Given this and idiosyncrasies associated with buyers, markets and properties, when we deliver new leads to agents, how many of those will eventually convert to transactions? How can we leverage existing data and experimentation to provide improved guidance on revenue sharing with Flex agents in different markets? How can we advise Finance and Operations partners on strategic decisions regarding the Flex program?\\n\\nTo that end, you will:\\n\\nMaintain and improve an existing production conversion model that supports our Flex Agent business. Proactively conduct hypothesis-focused deep dives and explorations to understand the seller journey and experience with Zillow Group products Design and execute experiments (ex: A/B tests) that measure impact of product changes. Frame experimentation problems, both statistically and within the business and customer context Present and interpret customer insights, metric development, testing methodologies, and experimentation results to technical and non-technical partners Work with engineering teams to improve data collection procedures to ensure integrity and quality of the data.\\n\\nThis role has been categorized as a Remote position. “Remote” employees do not have a permanent corporate office workplace and, instead, work from a physical location of their choice which must be identified to the Company. Employees may live in any of the 50 US States, with limited exceptions. In certain cases, an employee in a remote-designated job may need to live in a specific region or time zone to support customers or clients as part of their role.\\n\\nIn California, Colorado, Connecticut, Nevada, New York City and Washington the standard base pay range for this role is $140,900.00 - $225,100.00 Annually. This base pay range is specific to California, Colorado, Connecticut, Nevada, New York City and Washington and may not be applicable to other locations.\\n\\nIn addition to a competitive base salary this position is also eligible for equity awards based on factors such as experience, performance and location. Actual amounts will vary depending on experience, performance and location.\\n\\nWho you are\\n\\nAre you passionate about all things “home”? Would you describe yourself as a curious and ambitious problem solver? Come help us guide our business into the future with powerful insights and recommendations! We're seeking a data scientist with the follow qualifications:\\n\\nPossesses either an undergraduate or Master's degree in a quantitative field (e.g. mathematics, finance, statistics, or similar) or confirmed experience within data science and analytics 3+ years of work experience involving quantitative data analysis and complex problem solving Excellent communication skills with the ability to distill complex issues and detailed analysis into simple, structured frameworks with concrete action plans Experience building statistical models to yield insights from complex user journeys. Experience in maintaining and developing production-grade models is a must. Strong proficiency in Python and/or another programming language; experience using SQL, Tableau, Excel and Airflow. Experience in experimentation methodologies, causal inferences and pricing Strong product sense\\n\\nGet to know us\\n\\nZillow is reimagining real estate to make home a reality for more and more people.\\n\\nAs the most-visited real estate website in the United States, Zillow® and its affiliates help movers find and win their home through digital solutions, first class partners, and easier buying, selling, financing and renting experiences. Millions of people visit Zillow Group sites every month to start their home search, and now they can rely on Zillow to help make it easier to move. The work we do helps people get home and no matter what job you're in, you will play a critical role in making home a reality for more and more people.\\n\\nOur efforts to streamline the real estate transaction are supported by a deep-rooted culture of innovation, our passion to redefine the employee experience, a fundamental commitment to Equity and Belonging, and world-class benefits. These benefits include comprehensive medical, dental, vision, life, and disability coverages as well as parental leave, family benefits, retirement contributions, and paid time off. We’re also setting the standard for work experiences of the future, where our employees are supported in doing their best work and living a flexible, well-balanced life. But don’t just take our word for it. Read recent reviews on Glassdoor and recent recognition from multiple organizations, including: the 100 Best Companies to Work For, Glassdoor Employees’ Choice Award, Bloomberg Gender-Equality Index, Human Rights Campaign (HRC) Corporate Equity Index, and TIME 100 Most Influential Companies list.\\n\\nZillow Group is an equal opportunity employer committed to fostering an inclusive, innovative environment with the best employees. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. If you have a disability or special need that requires accommodation, please contact us at RecruitingAccessibility@zillowgroup.com.\\n\\nApplicants who receive job offers from Zillow Group will be asked to sign a Proprietary Rights Agreement which includes confidentiality, intellectual property assignment, customer and employee non-solicitation, and non-competition provisions. If you are contacted for a role at Zillow Group and wish to review a copy of the Proprietary Rights Agreement prior to receiving an offer, you may request a copy from your Recruiter.\", metadata={'source': 'doc/zillow-jd.txt'}),\n",
       " Document(page_content='(cid:211) 503-505-0595 | sejal.dua@tufts.edu | sejaldua (cid:135) sejaldua | § sejaldua.com\\n\\nSejal Dua\\n\\nEducation\\n\\nTufts University B.S. Data Science, B.S. Biomedical Engineering | GPA: 3.82 (Summa Cum Laude)\\n\\nJesuit High School GPA: 4.26 weighted, 3.96 unweighted\\n\\nMedford, MA Sep 2017 – May 2021 Portland, OR Sep 2013 – Jun 2017\\n\\nExperience\\n\\nNike - North America Nike Direct Stores Analytics Data Analytic Manager\\n\\nDec 2021 – Present Beaverton, OR\\n\\nEngineered a pipeline to collate a temporally dynamic 100+ feature dataset at the store grain; developed a stakeholder-facing Streamlit store clustering web application using K-Means Clustering, Gaussian Mixture Models, and Principal Feature Analysis to support merch, planning, and allocation decision-making (85% adoption rate)\\n\\nCreated a \"Win With Sport\" Tableau dashboard which was socialized throughout NA planning and assortment teams with the\\n\\nobjective of achieving optimal gear up and licensed product assortment coverage and productivity across key sports, leagues, and teams in stores; implemented flexible, weighted ranking methodology for aggregating multiple signals\\n\\nLaunched a post-store opening analysis framework (SQL, Python, Tableau) with the objective of empowering the Unite\\n\\nConcept team with early insights for new store openings throughout NA stores acceleration; leveraged Google’s Causal Impact Analysis model in order to quantify amount of business created and/or cannibalized by new store opening\\n\\nIBM Watson Health Data Scientist\\n\\nJun 2021 – Dec 2021 Cambridge, MA\\n\\nImplemented client-facing predictive model (3x better than baseline) for Medicaid enrollment forecasting; identified most\\n\\noptimal model across 5+ exogenous and non-exogenous time series forecasting approaches including Holt-Winters exponential smoothing, ARIMA, SARIMAX, and Keras models.\\n\\nCollaborated with Program Integrity analysts to develop parameterized PySpark algorithms for detection and further\\n\\ninvestigation of fraud, waste, and abuse patterns across providers.\\n\\nLeveraged Streamlit and Networkx libraries to build a dashboard illuminating patient-provider trends within MarketScan\\n\\nde-identified claims database, as discovered via graph algorithms.\\n\\nIBM Research Machine Learning Intern\\n\\nJun 2020 – Sep 2020 Yorktown Heights, NY\\n\\nEngineered primary NLP classification model for IBM Drug Repurposing for Cancer pipeline by performing corpus filtering\\n\\nthrough heuristic rules encapsulating domain expertise.\\n\\nAchieved 83% accuracy after training a Snorkel-based distantly supervised binary classifier on 127,000 unlabeled PubMed\\n\\narticles and validating on 1,400 labeled articles.\\n\\nTextbook Exchange Network Director of Data Analytics\\n\\nJan 2019 – Jan 2021 Medford, MA\\n\\nGathered data-driven insights from 6000+ API exchanges representing textbook transactions that have saved students\\n\\n$500,000 compared to campus bookstore prices.\\n\\nCalculated Key Performance Indicators (KPIs) via SQL queries and Python statistical methods to measure the health and\\n\\nwealth of the organization and market estimated impact.\\n\\nManaged a team of computer science students through numerous cycles of exploratory data analysis, feature integration,\\n\\nvisualization, and presentation at monthly showcases.\\n\\nProjects\\n\\nTechTogether Boston 2020: Pilter AI | Awards: IBM Best Hack & Dell Technologies Best Hack\\n\\nJan 2020 • Built an NLP-powered, user-facing annotation tool that sifts through abstracts from therapeutic cancer intervention studies\\n\\nand extracts relevant data using named entity recognition (NER).\\n\\nBeyond the Lyrics | Published in Towards Data Science\\n\\nPerformed sentiment analysis on song lyrics using the Spotify API, Python, and Tableau; Wrote 12 Medium articles reaching\\n\\nNov 2019\\n\\nover 60K readers in the data journalism space.\\n\\nTechnical Skills\\n\\nLanguages: Python, SQL, Spark, JavaScript, Java, C/C++, HTML/CSS, R, MATLAB, PHP, Bash Frameworks: React, React Native, Node.js, Flask, Tableau, Streamlit, Cognos, MongoDB, Django Developer Tools: Git, Jupyter Notebooks, Docker, Google Cloud Platform, Visual Studio Code, Xcode, PyCharm, IntelliJ Libraries: matplotlib, numpy, pandas, sklearn, statmodels, scipy, spacy, D3, Keras, Tensorflow, PyTorch', metadata={'source': 'doc/Sejal_Dua_Resume_2023.pdf'})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Sejal went to Tufts University.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=config.OPENAI_API_KEY)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(openai_api_key = config.OPENAI_API_KEY), chain_type=\"stuff\", vectorstore=docsearch)\n",
    "\n",
    "query = \"Where did Sejal go to college?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 09:56:39,129 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
      "/Users/sejaldua/Library/Python/3.8/lib/python/site-packages/langchain/chains/retrieval_qa/base.py:251: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The job posting requires 3+ years of work experience involving quantitative data analysis and complex problem solving.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=config.OPENAI_API_KEY)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(openai_api_key = config.OPENAI_API_KEY), chain_type=\"stuff\", vectorstore=docsearch)\n",
    "\n",
    "query = \"How many years of experience are required for the Zillow job posting?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 09:57:13,951 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
      "/Users/sejaldua/Library/Python/3.8/lib/python/site-packages/langchain/chains/retrieval_qa/base.py:251: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Yes, Sejal has 3+ years of work experience involving quantitative data analysis and complex problem solving.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=config.OPENAI_API_KEY)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(openai_api_key = config.OPENAI_API_KEY), chain_type=\"stuff\", vectorstore=docsearch)\n",
    "\n",
    "query = \"Does Sejal have 3+ years of work experience involving quantitative data analysis and complex problem solving.?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sejaldua/Library/Python/3.8/lib/python/site-packages/langchain/chains/retrieval_qa/base.py:251: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Sejal has built an NLP-powered annotation tool to sift through cancer intervention studies, performed sentiment analysis on song lyrics using the Spotify API, Python, and Tableau, and collected data-driven insights from 6000+ API exchanges representing textbook transactions.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(openai_api_key = config.OPENAI_API_KEY), chain_type=\"stuff\", vectorstore=docsearch)\n",
    "\n",
    "query = \"Summarize some of the projects Sejal has worked on.\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sejaldua/Library/Python/3.8/lib/python/site-packages/langchain/chains/retrieval_qa/base.py:251: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Sejal's high school GPA was 4.26 weighted and 3.96 unweighted; in college, her GPA was 3.82 (Summa Cum Laude).\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = VectorDBQA.from_chain_type(llm=OpenAI(openai_api_key = config.OPENAI_API_KEY), chain_type=\"stuff\", vectorstore=docsearch)\n",
    "query = \"What was Sejal's GPA in high school and college, respectively?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer: Sejal has coding skills in Python, SQL, Spark, JavaScript, Java, C/C++, HTML/CSS, R, MATLAB, PHP, Bash, React, React Native, Node.js, Flask, Tableau, Streamlit, Cognos, MongoDB, Django, Git, Jupyter Notebooks, Docker, Google Cloud Platform, Visual Studio Code, Xcode, PyCharm, IntelliJ, matplotlib, numpy, pandas, sklearn, statmodels, scipy, spacy, D3, Keras, Tensorflow, and PyTorch.\\n\\nI hope this answers your question. Is there anything else I can help you with?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import config\n",
    "from langchain import OpenAI,VectorDBQA\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "#import magic\n",
    "import os\n",
    "import nltk\n",
    "import config\n",
    "\n",
    "loader = DirectoryLoader('doc/', glob='*.pdf')\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=config.OPENAI_API_KEY)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "prompt_template = PromptTemplate(template=config.prompt_template, input_variables=[\"context\", \"question\"])\n",
    "doc_chain = load_qa_chain(\n",
    "    llm=OpenAI(\n",
    "        openai_api_key = config.OPENAI_API_KEY,\n",
    "        model_name=\"text-davinci-003\",\n",
    "        temperature=0,\n",
    "        max_tokens=300,\n",
    "    ),\n",
    "    chain_type=\"stuff\",\n",
    "    prompt=prompt_template,\n",
    ")\n",
    "qa = VectorDBQA(vectorstore=docsearch, combine_documents_chain=doc_chain, k=config.k)\n",
    "query = \"What coding skills does Sejal have?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 09:50:38,991 - INFO - Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import os\n",
    "import nltk\n",
    "import config\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(config.LOGS_FILE),\n",
    "        logging.StreamHandler(),\n",
    "    ],\n",
    ")\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "loader = DirectoryLoader(config.FILE_DIR, glob='*.pdf')\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=config.OPENAI_API_KEY)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# Define answer generation function\n",
    "def answer(prompt: str, persist_directory: str = config.PERSIST_DIR) -> str:\n",
    "    LOGGER.info(f\"Start answering based on prompt: {prompt}.\")\n",
    "    #vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    prompt_template = PromptTemplate(template=config.prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    doc_chain = load_qa_chain(\n",
    "        llm=OpenAI(\n",
    "            openai_api_key = config.OPENAI_API_KEY,\n",
    "            model_name=\"text-davinci-003\",\n",
    "            temperature=0,\n",
    "            max_tokens=300,\n",
    "        ),\n",
    "        chain_type=\"stuff\",\n",
    "        prompt=prompt_template,\n",
    "    )\n",
    "    LOGGER.info(f\"The top {config.k} chunks are considered to answer the user's query.\")\n",
    "    qa = VectorDBQA(vectorstore=docsearch, combine_documents_chain=doc_chain, k=config.k)\n",
    "    #qa = VectorDBQA.from_chain_type(llm=OpenAI(openai_api_key = config.OPENAI_API_KEY), chain_type=\"stuff\", vectorstore=docsearch)\n",
    "    result = qa({\"query\": prompt})\n",
    "    answer = result[\"result\"]\n",
    "    LOGGER.info(f\"The returned answer is: {answer}\")\n",
    "    LOGGER.info(f\"Answering module over.\")\n",
    "    return answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
